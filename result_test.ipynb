{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_highergov_opportunities(filepath):\n",
    "    \"\"\"\n",
    "    Load the HigherGov API file and extract opportunity identifiers.\n",
    "    Each opportunity is represented as a set containing:\n",
    "      - source_id\n",
    "      - source_id_version\n",
    "    Returns a list of sets.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        # Determine if data is wrapped in a dict or is a list of records.\n",
    "        if isinstance(data, dict):\n",
    "            records = data.get(\"results\", [])\n",
    "        elif isinstance(data, list):\n",
    "            records = data\n",
    "        else:\n",
    "            records = []\n",
    "    opportunities = []\n",
    "    for record in records:\n",
    "        ids = set()\n",
    "        if \"source_id\" in record:\n",
    "            ids.add(record[\"source_id\"])\n",
    "        if \"source_id_version\" in record:\n",
    "            ids.add(record[\"source_id_version\"])\n",
    "        opportunities.append(ids)\n",
    "    return opportunities\n",
    "\n",
    "def load_results_opportunities(filepath):\n",
    "    \"\"\"\n",
    "    Load the results JSON file and extract opportunity identifiers.\n",
    "    Each opportunity is represented as a set containing:\n",
    "      - solicitation_id (if it exists)\n",
    "      - notice_id (if it exists)\n",
    "      - solicitationNumber from any history records (if available)\n",
    "    Returns a list of sets.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    opportunities = []\n",
    "    for record in data:\n",
    "        ids = set()\n",
    "        if \"solicitation_id\" in record:\n",
    "            ids.add(record[\"solicitation_id\"])\n",
    "        if \"notice_id\" in record:  # Added to catch identifiers like \"1e3bb70baf844632878847c776199d92\".\n",
    "            ids.add(record[\"notice_id\"])\n",
    "        if \"history\" in record:\n",
    "            for hist in record[\"history\"]:\n",
    "                if \"solicitationNumber\" in hist:\n",
    "                    ids.add(hist[\"solicitationNumber\"])\n",
    "        opportunities.append(ids)\n",
    "    return opportunities\n",
    "\n",
    "def find_match(opportunity, other_opportunities):\n",
    "    \"\"\"\n",
    "    Given a set representing an opportunity, check if there is any opportunity in other_opportunities\n",
    "    which has a non-empty intersection with it.\n",
    "    If so, the opportunity is considered to have a match.\n",
    "    \"\"\"\n",
    "    for other in other_opportunities:\n",
    "        if opportunity.intersection(other):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    highergov_file = \"wnzTPS5NfNK5vVqRhbQ9i_highergov.json\"\n",
    "    results_file = \"wnzTPS5NfNK5vVqRhbQ9i_supabase.json\"\n",
    "\n",
    "    # Load opportunities from both files.\n",
    "    highergov_ops = load_highergov_opportunities(highergov_file)\n",
    "    results_ops = load_results_opportunities(results_file)\n",
    "    print(\"Highergov opportunities:\", highergov_ops)\n",
    "    \n",
    "    # Determine mismatches by checking each opportunity against the other file.\n",
    "    mismatch_records_highergov = [op for op in highergov_ops if not find_match(op, results_ops)]\n",
    "    mismatches_highergov = len(mismatch_records_highergov)\n",
    "    \n",
    "    mismatch_records_results = [op for op in results_ops if not find_match(op, highergov_ops)]\n",
    "    mismatches_results = len(mismatch_records_results)\n",
    "    \n",
    "    total_mismatches = mismatches_highergov + mismatches_results\n",
    "\n",
    "    print(\"----- Mismatch Report -----\")\n",
    "    print(\"Number of entries from highergov_api_full_data.json:\", len(highergov_ops))\n",
    "    print(\"Number of entries from results.json:\", len(results_ops))\n",
    "    print(\"Number of opportunities in highergov_api_full_data.json with no match in results.json:\", mismatches_highergov)\n",
    "    print(\"Number of opportunities in results.json with no match in highergov_api_full_data.json:\", mismatches_results)\n",
    "    print(\"Total mismatches:\", total_mismatches)\n",
    "    \n",
    "    # Print out the mismatch details\n",
    "    print(\"\\nDetailed Mismatches:\")\n",
    "    print(\"Mismatched opportunities from highergov_api_full_data.json:\")\n",
    "    for op in mismatch_records_highergov:\n",
    "        print(op)\n",
    "    \n",
    "    print(\"\\nMismatched opportunities from results.json:\")\n",
    "    for op in mismatch_records_results:\n",
    "        print(op)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def load_csv_opportunities(filepath):\n",
    "    \"\"\"\n",
    "    Load the contract opportunity CSV file and extract opportunity identifiers.\n",
    "    Each opportunity is represented as a set containing:\n",
    "      - source_id\n",
    "      - source_id_version\n",
    "    Returns a list of sets.\n",
    "    \"\"\"\n",
    "    opportunities = []\n",
    "    with open(filepath, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            #print(row)\n",
    "            ids = set()\n",
    "            # Check and add identifier fields if present and non-empty.\n",
    "            if 'Solicitation ID' in row and row['Solicitation ID']:\n",
    "                ids.add(row['Solicitation ID'])\n",
    "            if 'Solicitation Title' in row and row['Solicitation Title']:\n",
    "                ids.add(row['Solicitation Title'])\n",
    "            if '\\ufeffNotice ID' in row and row['\\ufeffNotice ID']:\n",
    "                #print(\"catch\")\n",
    "                ids.add(row['\\ufeffNotice ID'])\n",
    "            opportunities.append(ids)\n",
    "    return opportunities\n",
    "\n",
    "def load_results_opportunities(filepath):\n",
    "    \"\"\"\n",
    "    Load the supabase JSON file and extract opportunity identifiers.\n",
    "    Each opportunity is represented as a set containing:\n",
    "      - solicitation_id (if it exists)\n",
    "      - notice_id (if it exists)\n",
    "      - solicitationNumber from any history records (if available)\n",
    "    Returns a list of sets.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    opportunities = []\n",
    "    for record in data:\n",
    "        ids = set()\n",
    "        if \"solicitation_id\" in record and record[\"solicitation_id\"]:\n",
    "            ids.add(record[\"solicitation_id\"])\n",
    "        if \"notice_id\" in record and record[\"notice_id\"]:\n",
    "            ids.add(record[\"notice_id\"])\n",
    "        if \"title\" in record and record[\"title\"]:\n",
    "            ids.add(record[\"title\"])\n",
    "        if \"history\" in record:\n",
    "            for hist in record[\"history\"]:\n",
    "                if \"solicitationNumber\" in hist and hist[\"solicitationNumber\"]:\n",
    "                    ids.add(hist[\"solicitationNumber\"])\n",
    "        opportunities.append(ids)\n",
    "    return opportunities\n",
    "\n",
    "def find_match(opportunity, other_opportunities):\n",
    "    \"\"\"\n",
    "    Given a set representing an opportunity, check if there is any opportunity in other_opportunities\n",
    "    that has a non-empty intersection with it.\n",
    "    If so, the opportunity is considered to have a match.\n",
    "    \"\"\"\n",
    "    for other in other_opportunities:\n",
    "        if opportunity.intersection(other):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    csv_file = \"contract_opportunity-03-17-25-18-48-13.csv\"\n",
    "    results_file = \"wnzTPS5NfNK5vVqRhbQ9i_results.json\"\n",
    "\n",
    "    # Load opportunities from both files.\n",
    "    csv_ops = load_csv_opportunities(csv_file)\n",
    "    results_ops = load_results_opportunities(results_file)\n",
    "    #print(\"CSV opportunities:\", csv_ops)\n",
    "    \n",
    "    # Determine mismatches by checking each opportunity against the other file.\n",
    "    mismatch_records_csv = [op for op in csv_ops if not find_match(op, results_ops)]\n",
    "    mismatches_csv = len(mismatch_records_csv)\n",
    "    \n",
    "    mismatch_records_results = [op for op in results_ops if not find_match(op, csv_ops)]\n",
    "    mismatches_results = len(mismatch_records_results)\n",
    "    \n",
    "    total_mismatches = mismatches_csv + mismatches_results\n",
    "\n",
    "    print(\"----- Mismatch Report -----\")\n",
    "    print(\"Number of entries from CSV file:\", len(csv_ops))\n",
    "    print(\"Number of entries from results.json:\", len(results_ops))\n",
    "    print(\"Opportunities in highergov CSV with no match in supabase:\", mismatches_csv)\n",
    "    print(\"Opportunities in supabase with no match in highergov CSV:\", mismatches_results)\n",
    "    print(\"Total mismatches:\", total_mismatches)\n",
    "    \n",
    "    # Print out the mismatch details\n",
    "    print(\"\\nDetailed Mismatches:\")\n",
    "    print(\"Mismatched opportunities from CSV file:\")\n",
    "    for op in mismatch_records_csv:\n",
    "        print(op)\n",
    "    \n",
    "    print(\"\\nMismatched opportunities from results.json:\")\n",
    "    for op in mismatch_records_results:\n",
    "        print(op)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying active filter\n",
      "Current time: 2025-03-17T20:19:25.867510+00:00\n",
      "Including NAICS codes: ['237310', '488111', '493110', '518210', '541330', '541511', '541512', '541519', '541611', '541614', '562910', '513210', '516210', '561210', '221330', '236210', '236220', '237110', '237990', '238110', '238350', '238910', '238990', '333922', '541219', '541350', '541360', '541370', '541380', '541430', '541513', '541612', '541618', '541620', '541690', '541713', '541714', '541715', '541810', '541820', '541830', '541840', '541850', '541860', '541870', '541890', '541990', '561410', '561421', '561499', '561730', '561790', '611430']\n",
      "Excluding solicitation types:  ['s', 'a', 'u', 'j', 'l', 'm', 'g', 'f']\n",
      "Excluding set_aside_ids: [16, 24, 17, 18, 19, 20, 22, 21, 23]\n",
      "Applying active filter\n",
      "Current time: 2025-03-17T20:19:27.009057+00:00\n",
      "Including NAICS codes: ['237310', '488111', '493110', '518210', '541330', '541511', '541512', '541519', '541611', '541614', '562910', '513210', '516210', '561210', '221330', '236210', '236220', '237110', '237990', '238110', '238350', '238910', '238990', '333922', '541219', '541350', '541360', '541370', '541380', '541430', '541513', '541612', '541618', '541620', '541690', '541713', '541714', '541715', '541810', '541820', '541830', '541840', '541850', '541860', '541870', '541890', '541990', '561410', '561421', '561499', '561730', '561790', '611430']\n",
      "Excluding solicitation types:  ['s', 'a', 'u', 'j', 'l', 'm', 'g', 'f']\n",
      "Excluding set_aside_ids: [16, 24, 17, 18, 19, 20, 22, 21, 23]\n",
      "Applying active filter\n",
      "Current time: 2025-03-17T20:19:27.674169+00:00\n",
      "Including NAICS codes: ['237310', '488111', '493110', '518210', '541330', '541511', '541512', '541519', '541611', '541614', '562910', '513210', '516210', '561210', '221330', '236210', '236220', '237110', '237990', '238110', '238350', '238910', '238990', '333922', '541219', '541350', '541360', '541370', '541380', '541430', '541513', '541612', '541618', '541620', '541690', '541713', '541714', '541715', '541810', '541820', '541830', '541840', '541850', '541860', '541870', '541890', '541990', '561410', '561421', '561499', '561730', '561790', '611430']\n",
      "Excluding solicitation types:  ['s', 'a', 'u', 'j', 'l', 'm', 'g', 'f']\n",
      "Excluding set_aside_ids: [16, 24, 17, 18, 19, 20, 22, 21, 23]\n",
      "Number of results:  1365\n",
      "Saved results to wnzTPS5NfNK5vVqRhbQ9i_result.json file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "from supabase import create_client, Client\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def get_filtered_notices(\n",
    "    supabase: Client,\n",
    "    active: bool = True,\n",
    "    include_naics: Optional[List[str]] = None,\n",
    "    exclude_naics: Optional[List[str]] = None,\n",
    "    include_solicitation_types: Optional[List[str]] = None,\n",
    "    exclude_solicitation_types: Optional[List[str]] = None,\n",
    "    include_psc: Optional[List[str]] = None,\n",
    "    exclude_psc: Optional[List[str]] = None,\n",
    "    include_set_aside_ids: Optional[List[str]] = None,  # new filter: include by set_aside_id\n",
    "    exclude_set_aside_ids: Optional[List[str]] = None,  # new filter: exclude by set_aside_id\n",
    "    keyword_query: Optional[str] = None\n",
    ") -> List[Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Retrieve rows from the 'notices' table applying the provided filters and paginating through \n",
    "    all the available data.\n",
    "    \"\"\"\n",
    "    all_notices = []\n",
    "    limit = 1000  # Batch size for pagination.\n",
    "    offset = 0\n",
    "\n",
    "    while True:\n",
    "        # Build the base query with embedded joins.\n",
    "        query = supabase.from_(\"notices\").select(\"\"\"\n",
    "            *,\n",
    "            naics_details:naics!naics_id(*),\n",
    "            psc_details:psc!psc_id(*),\n",
    "            setasides_details:setasides!set_aside_id(*),\n",
    "            solicitations_details:solicitations!solicitation_id(*),\n",
    "            addresses_details:addresses!organization_address_key(*),\n",
    "            organization_details:organizations!Notices_organization_key_fkey(*),\n",
    "            organization_level_1_details:organizations!Notices_organization_level_1_key_fkey(*),\n",
    "            organization_level_2_details:organizations!Notices_organization_level_2_key_fkey(*),\n",
    "            organization_level_3_details:organizations!Notices_organization_level_3_key_fkey(*),\n",
    "            organization_level_4_details:organizations!Notices_organization_level_4_key_fkey(*),\n",
    "            organization_level_5_details:organizations!Notices_organization_level_5_key_fkey(*),\n",
    "            organization_level_6_details:organizations!Notices_organization_level_6_key_fkey(*),\n",
    "            organization_level_7_details:organizations!Notices_organization_level_7_key_fkey(*)\n",
    "        \"\"\")\n",
    "\n",
    "        if active:\n",
    "            print(\"Applying active filter\")\n",
    "            query = query.eq(\"latest\", True)\n",
    "            current_time = datetime.now(timezone.utc).isoformat()\n",
    "            print(\"Current time:\", current_time)\n",
    "            # Only include notices where the solicitation_response_deadline is in the future.\n",
    "            query = query.gt(\"solicitation_response_deadline\", current_time)\n",
    "\n",
    "        # Apply include filters for NAICS codes.\n",
    "        if include_naics:\n",
    "            print(\"Including NAICS codes:\", include_naics)\n",
    "            query = query.in_(\"naics\", include_naics)\n",
    "        # Apply exclude filters for NAICS codes.\n",
    "        if exclude_naics:\n",
    "            for code in exclude_naics:\n",
    "                query = query.neq(\"naics\", code)\n",
    "\n",
    "        # Apply filters for solicitation types.\n",
    "        if include_solicitation_types:\n",
    "            query = query.in_(\"type\", include_solicitation_types)\n",
    "        if exclude_solicitation_types:\n",
    "            print(\"Excluding solicitation types: \", exclude_solicitation_types)\n",
    "            for s_type in exclude_solicitation_types:\n",
    "                query = query.neq(\"type\", s_type)\n",
    "\n",
    "        # Apply filters for PSC codes.\n",
    "        if include_psc:\n",
    "            query = query.in_(\"psc\", include_psc)\n",
    "        if exclude_psc:\n",
    "            for psc in exclude_psc:\n",
    "                query = query.neq(\"psc\", psc)\n",
    "\n",
    "         # Apply filters for set aside IDs from the joined setasides table.\n",
    "        if include_set_aside_ids:\n",
    "            # Convert provided set_aside_ids to integer type to match int8 column in database.\n",
    "            include_set_aside_ids = [int(x) for x in include_set_aside_ids]\n",
    "            print(\"Including set_aside_ids:\", include_set_aside_ids)\n",
    "            query = query.in_(\"setasides_details.set_aside_id\", include_set_aside_ids)\n",
    "        if exclude_set_aside_ids:\n",
    "            # Convert provided set_aside_ids to integer type to match int8 column in database.\n",
    "            exclude_set_aside_ids = [int(x) for x in exclude_set_aside_ids]\n",
    "            print(\"Excluding set_aside_ids:\", exclude_set_aside_ids)\n",
    "            for set_aside_id in exclude_set_aside_ids:\n",
    "                query = query.neq(\"setasides_details.set_aside_id\", set_aside_id)\n",
    "           \n",
    "\n",
    "        # Apply an advanced keyword search on text fields.\n",
    "        if keyword_query:\n",
    "            or_filter = f\"title.fts.{keyword_query},description_body.fts.{keyword_query}\"\n",
    "            query = query.or_(or_filter)\n",
    "\n",
    "        # Apply pagination for the current batch.\n",
    "        query = query.range(offset, offset + limit - 1)\n",
    "        result = query.execute()\n",
    "\n",
    "        # If no more data is returned, exit the loop.\n",
    "        if not result.data:\n",
    "            break\n",
    "\n",
    "        all_notices.extend(result.data)\n",
    "        offset += limit\n",
    "\n",
    "    return all_notices\n",
    "\n",
    "def main():\n",
    "    load_dotenv()\n",
    "    SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "    SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "\n",
    "    if not SUPABASE_URL or not SUPABASE_KEY:\n",
    "        raise ValueError(\"SUPABASE_URL and SUPABASE_KEY environment variables must be set\")\n",
    "\n",
    "    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "    # Define filter parameters.\n",
    "    active = True\n",
    "    #active = False\n",
    "    include_naics = [\n",
    "        \"237310\", \"488111\", \"493110\", \"518210\", \"541330\", \"541511\", \"541512\", \"541519\",\n",
    "        \"541611\", \"541614\", \"562910\", \"513210\", \"516210\", \"561210\", \"221330\", \"236210\",\n",
    "        \"236220\", \"237110\", \"237990\", \"238110\", \"238350\", \"238910\", \"238990\", \"333922\",\n",
    "        \"541219\", \"541350\", \"541360\", \"541370\", \"541380\", \"541430\", \"541513\", \"541612\",\n",
    "        \"541618\", \"541620\", \"541690\", \"541713\", \"541714\", \"541715\", \"541810\", \"541820\",\n",
    "        \"541830\", \"541840\", \"541850\", \"541860\", \"541870\", \"541890\", \"541990\", \"561410\",\n",
    "        \"561421\", \"561499\", \"561730\", \"561790\", \"611430\"\n",
    "    ]\n",
    "\n",
    "    exclude_naics = []\n",
    "\n",
    "    include_solicitation_types = []      # types to include\n",
    "    #exclude_solicitation_types = []\n",
    "    exclude_solicitation_types = [\"s\", \"a\", \"u\", \"j\", \"l\", \"m\", \"g\", \"f\"]  # types to exclude\n",
    "    \n",
    "    exclude_psc = []\n",
    "    include_psc = []\n",
    "    \n",
    "    # Filters for set aside codes and IDs (from the joined setasides table).\n",
    "    \n",
    "    include_set_aside_ids = []  \n",
    "    #exclude_set_aside_ids = []\n",
    "    exclude_set_aside_ids = [16, 24, 17, 18, 19, 20, 22, 21, 23 ]  # example set_aside_id values to exclude\n",
    "\n",
    "    keyword_query = None\n",
    "\n",
    "    # Retrieve notices with the new filtering options.\n",
    "    notices = get_filtered_notices(\n",
    "        supabase=supabase,\n",
    "        active=active,\n",
    "        include_naics=include_naics,\n",
    "        exclude_naics=exclude_naics,\n",
    "        include_solicitation_types=include_solicitation_types,\n",
    "        exclude_solicitation_types=exclude_solicitation_types,\n",
    "        include_psc=include_psc,\n",
    "        exclude_psc=exclude_psc,\n",
    "        include_set_aside_ids=include_set_aside_ids,\n",
    "        exclude_set_aside_ids=exclude_set_aside_ids,\n",
    "        keyword_query=keyword_query\n",
    "    )\n",
    "\n",
    "    #print(json.dumps(notices, indent=2))\n",
    "    print(\"Number of results: \", len(notices))\n",
    "    with open(\"wnzTPS5NfNK5vVqRhbQ9i_results.json\", \"w\") as f:\n",
    "        json.dump(notices, f, indent=2, default=str)\n",
    "    print(\"Saved results to wnzTPS5NfNK5vVqRhbQ9i_result.json file\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "from supabase import create_client, Client\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def get_filtered_notices(\n",
    "    supabase: Client,\n",
    "    active: bool = True,\n",
    "    include_naics: Optional[List[str]] = None,\n",
    "    exclude_naics: Optional[List[str]] = None,\n",
    "    include_solicitation_types: Optional[List[str]] = None,\n",
    "    exclude_solicitation_types: Optional[List[str]] = None,\n",
    "    include_psc: Optional[List[str]] = None,\n",
    "    exclude_psc: Optional[List[str]] = None,\n",
    "    include_set_aside_ids: Optional[List[str]] = None,  # new filter: include by set_aside_id\n",
    "    exclude_set_aside_ids: Optional[List[str]] = None,  # new filter: exclude by set_aside_id\n",
    "    include_organization_keys: Optional[List[str]] = None,  # new filter: include by organization_key (int8)\n",
    "    exclude_organization_keys: Optional[List[str]] = None,  # new filter: exclude by organization_key (int8)\n",
    "    keyword_query: Optional[str] = None\n",
    ") -> List[Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Retrieve rows from the 'notices' table applying the provided filters and paginating through all the available data.\n",
    "    \"\"\"\n",
    "    all_notices = []\n",
    "    limit = 1000  # Batch size for pagination.\n",
    "    offset = 0\n",
    "\n",
    "    while True:\n",
    "        # Build the base query with embedded joins.\n",
    "        query = supabase.from_(\"notices\").select(\"\"\"\n",
    "            *,\n",
    "            naics_details:naics!naics_id(*),\n",
    "            psc_details:psc!psc_id(*),\n",
    "            setasides_details:setasides!set_aside_id(*),\n",
    "            solicitations_details:solicitations!solicitation_id(*),\n",
    "            addresses_details:addresses!organization_address_key(*),\n",
    "            organization_details:organizations!Notices_organization_key_fkey(*),\n",
    "            organization_level_1_details:organizations!Notices_organization_level_1_key_fkey(*),\n",
    "            organization_level_2_details:organizations!Notices_organization_level_2_key_fkey(*),\n",
    "            organization_level_3_details:organizations!Notices_organization_level_3_key_fkey(*),\n",
    "            organization_level_4_details:organizations!Notices_organization_level_4_key_fkey(*),\n",
    "            organization_level_5_details:organizations!Notices_organization_level_5_key_fkey(*),\n",
    "            organization_level_6_details:organizations!Notices_organization_level_6_key_fkey(*),\n",
    "            organization_level_7_details:organizations!Notices_organization_level_7_key_fkey(*),\n",
    "            solicitation_type_details:solicitation_types!type(*)\n",
    "        \"\"\")\n",
    "\n",
    "        if active:\n",
    "            print(\"Applying active filter\")\n",
    "            query = query.eq(\"latest\", True)\n",
    "            current_time = datetime.now(timezone.utc).isoformat()\n",
    "            print(\"Current time:\", current_time)\n",
    "            # Only include notices with a future solicitation_response_deadline.\n",
    "            query = query.gt(\"solicitation_response_deadline\", current_time)\n",
    "\n",
    "        if include_naics:\n",
    "            print(\"Including NAICS codes:\", include_naics)\n",
    "            query = query.in_(\"naics\", include_naics)\n",
    "        if exclude_naics:\n",
    "            for code in exclude_naics:\n",
    "                query = query.neq(\"naics\", code)\n",
    "\n",
    "        if include_solicitation_types:\n",
    "            query = query.in_(\"type\", include_solicitation_types)\n",
    "        if exclude_solicitation_types:\n",
    "            print(\"Excluding solicitation types:\", exclude_solicitation_types)\n",
    "            for s_type in exclude_solicitation_types:\n",
    "                query = query.neq(\"type\", s_type)\n",
    "\n",
    "        if include_psc:\n",
    "            query = query.in_(\"psc\", include_psc)\n",
    "        if exclude_psc:\n",
    "            for psc in exclude_psc:\n",
    "                query = query.neq(\"psc\", psc)\n",
    "\n",
    "        if include_set_aside_ids:\n",
    "            # Convert set_aside_ids to integers for matching database type.\n",
    "            include_set_aside_ids = [int(x) for x in include_set_aside_ids]\n",
    "            print(\"Including set_aside_ids:\", include_set_aside_ids)\n",
    "            query = query.in_(\"setasides_details.set_aside_id\", include_set_aside_ids)\n",
    "        if exclude_set_aside_ids:\n",
    "            # Convert set_aside_ids to integers for matching database type.\n",
    "            exclude_set_aside_ids = [int(x) for x in exclude_set_aside_ids]\n",
    "            print(\"Excluding set_aside_ids:\", exclude_set_aside_ids)\n",
    "            for set_aside_id in exclude_set_aside_ids:\n",
    "                query = query.neq(\"setasides_details.set_aside_id\", set_aside_id)\n",
    "                \n",
    "        if include_organization_keys:\n",
    "            # Convert organization_keys to integers for matching database type.\n",
    "            include_organization_keys = [int(x) for x in include_organization_keys]\n",
    "            print(\"Including organization_keys:\", include_organization_keys)\n",
    "            query = query.in_(\"organization_details.organization_key\", include_organization_keys)\n",
    "        if exclude_organization_keys:\n",
    "            # Convert organization_keys to integers for matching database type.\n",
    "            exclude_organization_keys = [int(x) for x in exclude_organization_keys]\n",
    "            print(\"Excluding organization_keys:\", exclude_organization_keys)\n",
    "            for org_key in exclude_organization_keys:\n",
    "                query = query.neq(\"organization_details.organization_key\", org_key)\n",
    "\n",
    "        query = query.range(offset, offset + limit - 1)\n",
    "        \n",
    "        if keyword_query:\n",
    "            print(\"Applying keyword search filter:\", keyword_query)\n",
    "            # Use full-text search on the \"opportunity_text\" field.\n",
    "            query = query.text_search(\"opportunity_text\", keyword_query, {'config': 'english', 'type': 'websearch'})\n",
    "            \n",
    "        result = query.execute()\n",
    "        \n",
    "        # Exit loop if no more data is returned.\n",
    "        if not result.data:\n",
    "            break\n",
    "\n",
    "        all_notices.extend(result.data)\n",
    "        offset += limit\n",
    "\n",
    "    return all_notices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    load_dotenv()\n",
    "    SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "    SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "\n",
    "    if not SUPABASE_URL or not SUPABASE_KEY:\n",
    "        raise ValueError(\"SUPABASE_URL and SUPABASE_KEY environment variables must be set\")\n",
    "\n",
    "    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "    # Define filter parameters.\n",
    "    active = True\n",
    "    include_naics = [\n",
    "        \"237310\", \"488111\", \"493110\", \"518210\", \"541330\", \"541511\", \"541512\", \"541519\",\n",
    "        \"541611\", \"541614\", \"562910\", \"513210\", \"516210\", \"561210\", \"221330\", \"236210\",\n",
    "        \"236220\", \"237110\", \"237990\", \"238110\", \"238350\", \"238910\", \"238990\", \"333922\",\n",
    "        \"541219\", \"541350\", \"541360\", \"541370\", \"541380\", \"541430\", \"541513\", \"541612\",\n",
    "        \"541618\", \"541620\", \"541690\", \"541713\", \"541714\", \"541715\", \"541810\", \"541820\",\n",
    "        \"541830\", \"541840\", \"541850\", \"541860\", \"541870\", \"541890\", \"541990\", \"561410\",\n",
    "        \"561421\", \"561499\", \"561730\", \"561790\", \"611430\"\n",
    "    ]\n",
    "    exclude_naics = []\n",
    "\n",
    "    include_solicitation_types = []  # types to include\n",
    "    exclude_solicitation_types = [\"s\", \"a\", \"u\", \"j\", \"l\", \"m\", \"g\", \"f\"]  # types to exclude\n",
    "    \n",
    "    include_psc = []\n",
    "    exclude_psc = []\n",
    "    \n",
    "    # Filters for set aside codes and IDs.\n",
    "    include_set_aside_ids = []  \n",
    "    exclude_set_aside_ids = [16, 24, 17, 18, 19, 20, 22, 21, 23]  # example set_aside_id values to exclude\n",
    "\n",
    "    # New filters for organization keys (int8).\n",
    "    include_organization_keys = []  # example organization keys to include (int8)\n",
    "    exclude_organization_keys = []  # example organization keys to exclude (int8)\n",
    "\n",
    "    keyword_query = \"ITAD | media | destruction | digital | media | destruction\"\n",
    "\n",
    "    # Retrieve notices using the specified filters.\n",
    "    notices = get_filtered_notices(\n",
    "        supabase=supabase,\n",
    "        active=active,\n",
    "        include_naics=include_naics,\n",
    "        exclude_naics=exclude_naics,\n",
    "        include_solicitation_types=include_solicitation_types,\n",
    "        exclude_solicitation_types=exclude_solicitation_types,\n",
    "        include_psc=include_psc,\n",
    "        exclude_psc=exclude_psc,\n",
    "        include_set_aside_ids=include_set_aside_ids,\n",
    "        exclude_set_aside_ids=exclude_set_aside_ids,\n",
    "        include_organization_keys=include_organization_keys,\n",
    "        exclude_organization_keys=exclude_organization_keys,\n",
    "        keyword_query=keyword_query\n",
    "    )\n",
    "\n",
    "    print(\"Number of results:\", len(notices))\n",
    "    with open(\"wnzTPS5NfNK5vVqRhbQ9i_results.json\", \"w\") as f:\n",
    "        json.dump(notices, f, indent=2, default=str)\n",
    "    print(\"Saved results to wnzTPS5NfNK5vVqRhbQ9i_result.json file\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
